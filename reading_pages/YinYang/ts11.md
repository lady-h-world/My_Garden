## Outlier Detection

If we take a brief looking back, trend detection and changepoint detection in both Kats and Greykite all return detected data points. Besides, any other data points that are worthy to explore?

One of an interesting concept is the outliers in time series. As you saw in [Queen Lotus][1], an outlier is an object that's quite diffrent from the rest of the data. In time series, such data point can be insightful and interesting to explore, such as what caused the data peaks in fraud detection traffic? why Sunday sales is always lower than other weekdays? etc., etc., there are many other fun examples.

Some outliers can be ignored, some should be taken into consideration in later modeling stage. Therefore, it's often a good practice to understand the causes, then decide whether to remove the outliers.

In this section, you will not only see Kats and Greykite outlier detection, but also see Lady H.'s self-implemented methods! ðŸ˜‰

### Kats Outlier Detection (Univariate Time Series)

Kats outlier detection for univariate time series is based on `n * IQR` idea, if a value is `n * IQR` lower than Q1 (quantile 1) or `n * IQR` higher than Q3 (quantile 3), then this value will be considered as an outlier, you can adjust the value of `n`.

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/notes/iqr.png" width="766" height="79" />
</p>

More detailed process is, Kats will decompose the time series first using additive or multiplicative decomposition, its purpose is to remove trend, seasonality and only keep residuals, then it detects outliers in residuals using the `n * IQR` method. By default, Kats uses `n=3` but you can adjust its value with parameter `iqr_mult`.

The detection code is as simple as the 2 lines of code below. The time series here is sales data which was proved to be a better fit for multiplication decomposition, therefore here we are using "multiplication" as the decomposing method, set `iqr_mult` as 5 so that we can get more obvious outliers.

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/kats_outlier_code.png" width="686" height="46" />
</p>

If we plot the detected outliers on the original time series data, the detected outliers are not all appear at crest or trough. However, they do appear to be more different from the rest of the data in residuals.

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/kats_outliers.png" width="1203" height="313" />
</p>

This also tells us, sometimes the crest or trough of a time series may not be an outlier even if they have the most extreme values, they might be caused by seasonality or trend.

At the same time, Kats provides outlier removal function, and there are 2 options:
* "No interpolation" option will replace detected outliers with NAN
* "With interpolation" option will replace detected outliers using [linear interpolation][3] values

In the charts below, Lady H. has plotted the results from both options, the yellow line is the original time series while the green line is the time series after Kats outlier detection:

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/kats_outlier_removal.png" width="1203" height="313" />
</p>

Again, we need to be cautious about outliers removal in time series data, since keeping some insightful outliers for later model forecasting will bring us more benefits than removing them.

ðŸŒ» [Check detailed code in Kats Outlier Detection >>][4]


### Lady H.'s Self Implemented Outlier Detection (Multivariate Time Series)

Although Kats provides multivariate time series detection, it simply didn't work. Check [this chat][2], you will see the errors Lady H. got in Kats multivariate outlier detection, and the evidence that it was caused by the bug in Kats. After checking the basic logic of Kats multivariate outlier detection, Lady H. realized, it's more efficient to implement the method herself than debugging Kats. Besides, she implemented more solutions for multivariate time series' outlier detection! ðŸ˜‰

#### VAR for Multivariate Time Series Outlier Detection

Kats was using VAR (Vector Auto Regression) for multivariate time series' outlier detection. Lady H. made some improvement upon this idea, and the general process is:

1. Make sure every time series in the input is stationary
2. Fit VAR model with an optimal order, and apply Durbin Watson test to make sure there is no leftover pattern in the residuals in any time series
3. Calculate squared errors with the fitted VAR, then calculate a threshold as `threshold = avg + n * std`
4. The outliers are data records with its squared error higher than the threshold

Now let's dive into more details!

First of all, we need to understand how does VAR work. VAR is a statistical model used for capturing the relationship between multiple quantities as they change over time. In VAR model, each variable is modeled as a linear combination of past values of itself and the past values of other variables in the system. Since you have multiple time series that influence each other, it is modeled as a system of equations with one equation per variable (time series). To help you understand its formula better, Lady H. provided 3 examples below.

* In the 1st example, there are 2 variables `Y1` and `Y2`, each variable is represented as a linear combination of its lags (past values) and other variables' lags. `Î±` is the intercept, `Î²` is the coefficient, `Îµ` is the error term. The number of lags is the order of VAR, so in this example the past values are only using `t-1`, so the order is 1.
* Similarily, in the 2nd example, the order is 2 because each variable has used `t-1`, `t-2`, 2 lags.
* In the 3rd exaample, the VAR order is still 2, but there are 3 variables `Y1`, `Y2`, `Y3`.

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/var_examples.png" width="863" height="319" />
</p>

Before applying VAR, we need to convert all the input variables to stationary, this is the premise of VAR. In fact we have already done this in previous data exploration section, after doing the first order differencing on "Humidity" and "HumidityRatio", we can get each time series to satisfy both differencing stationary and trending stationary.

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/stationary_multi_ts.png" width="524" height="234" />
</p>

Next, we will find the optimal order of VAR. VAR uses 4 model selection metrics to decide the optimal order:
* AIC (Akaike's Information Criterion) is an estimator of prediction error of any estimated statistical model. It uses a penalty term when exceeding the optimal number of parameters. AIC generally tries to find unknown model that has high dimensional reality, which also means the models are not true models in AIC.
* BIC (Bayesian Information Criteria) does model selection among a class of parametric models with different numbers of parameters. Comparing with AIC, BIC only come across true models and penalizes additional parameters stronger.
* FPE (Final Prediction Error) estimates the model-fitting error when you use the model to predict new outputs. A model with a lower FPE has a better balance between the number of parameters and the explained variation.
* HQIC (Hannanâ€“Quinn information criterion) also has a penality term as AIC but the penality is stronger.

When the value of each metric is lower, it indicates the model works better.

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/var_opt_order.png" width="379" height="791" />
</p>

As we saw from the output above, when the order is 18, it gets lowest AIC and FPE, BIC and HQIC are almost the lowest too. Therefore, the optimal order was selected as 18. But getting VAR selected optimal order is not enough, we need to ensure there is no leftover pattern in the residuals. To do this, we use Durbin Watson, it checks the serial correlation to ensure that the model is able to explain the variances and patterns in the time series, without leaving leftover patter in the residuals. Durbin Watson test has its output between [0, 4], when the value is close to 2, then there is no significant serial correlation. Closer to 0 indicates a positive serial correlation and closer to 4 implies a negative serial correlation. 

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/durbin_watson_test.png" width="871" height="233" />
</p>

The output above looks good. Otherwise, more actions are needed, such as increasing the order of the model or adding more variables or looking for a different model.

Now it comes to the core logic of outlier detection, which only has 2 lines of code as shown in `detect_anomalies()` function. The input is the squared error of the fitted VAR, then a threshold is calculated using the average squared error plus `n` times of the standard deviation of squared error. An outlier is the record with its squared error reaches to or above the threshold.

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/multi_ts_outlier_detection.png" width="530" height="92" />
</p>

The whole piece of code and sample output is shown like this:

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/var_outliers.png" width="697" height="408" />
</p>

However how to visualize the the outliers of this multivariate time series still puzzles Lady H., since if you [look at each time series in the data][5], they have quite different scales, we can't simply put them in 1 plot and show the outliers, put them in seperate plots and show outliers still can't deliver any meaningful insight since the idea is based on the relationship between multiple time series. Any suggestion on the visualization in this problem? [You are more than welcome to contribute ideas here!][6]

ðŸŒ» [Check detailed code in VAR Outlier Detection >>][4]

#### VECM for Multivariate Time Series Outlier Detection

VECM is known as Vector Error Correction Model. It adds error correction features to VAR. To form the analysis, VECM will estimate an unrestricted VAR involving potentially non-stationary variables, and then test cointegration using Johansen test. Therefore, VECM is also considered as a cointegrated VAR. The advantage of VECM over VAR is, VECM has more efficient coefficient estimates.

To apply VECM in multivariate time series outlier detection, the code is quite similar to VAR outlier detection, we just need to replace VAR model with VECM ðŸ˜‰

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/vecm_outlier_detection.png" width="619" height="669" />
</p>

ðŸŒ» [Check detailed code in VECM Outlier Detection >>][4]

After applying VAR and VECM in outlier detection, are you curious about how different the detection output will be? Lady H. was also quite curious about it and did a comparison. In this problem, among all the 8143 records, only 2 of them had different outlier detection results.

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/var_vs_vecm.png" width="427" height="114" />
</p>


### Greykite Outlier Detection

Have to admit, Greykite doesn't have a mature outlier detection method. It only has `ZscoreOutlierTransformer()` function which will replace its detected outliers to NAN directly, this can be risky, since some outliers will benefit later model forecasting and better not to be removed. Moreover, it only applies to univariate time series.

Greykite detects outliers based on z-score cutoff idea:
1. It first calculates the z-score for each observation using `z-score = abs(observation_value - population_mean) / population_std`
2. If a z-score value is larger than a cutoff threshold, then the observation will be considered as an outlier, and Greykite will repalce it to NAN

Here's Greykite detected outliers if we set the cutoff thereshold as 3. 

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/gk_outlier.png" width="1170" height="171" />
</p>

Let's compare with Kats detected outliers output:

<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/Garden_Totem_images/detection/kats_outliers.png" width="1203" height="313" />
</p>

Greykite tends to choose the most extreme data points as outliers, but after removing the trend and seasonality, this data point didn't appear to be an outlier in the residuals. For univariate time series' outlier detection, should we often use residuals instead of the raw time series for the detection? Kats vs Greykite method, which one makes more sense to you? [You are more than welcome to contribute ideas here!][6]

ðŸŒ» [Check detailed code in Greykite Outlier Detection >>][9]


#
<p align="left">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/follow_us.png" width="120" height="50" />
</p>

[Keep going >>][7]

<p align="right">
<img src="https://github.com/lady-h-world/My_Garden/blob/main/images/going_back.png" width="60" height="44" />
</p>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[<< Looking back][8]



[1]:https://github.com/lady-h-world/My_Garden/blob/main/reading_pages/the_queen.md#the-outlier
[2]:https://github.com/facebookresearch/Kats/issues/194
[3]:https://en.wikipedia.org/wiki/Interpolation#Linear_interpolation
[4]:https://github.com/lady-h-world/My_Garden/blob/main/code/yinyang/kats_experiments/kats_detect_outliers.ipynb
[5]:https://github.com/lady-h-world/My_Garden/blob/main/code/crystal_ball/data_collector/generate_multivariate_ts.ipynb
[6]:https://github.com/lady-h-world/My_Garden/discussions/categories/ideas
[7]:https://github.com/lady-h-world/My_Garden/blob/main/reading_pages/YinYang/ts12.md
[8]:https://github.com/lady-h-world/My_Garden/blob/main/reading_pages/YinYang/ts10.md
[9]:https://github.com/lady-h-world/My_Garden/blob/main/code/yinyang/greykite_experiments/gk_outliers.ipynb
